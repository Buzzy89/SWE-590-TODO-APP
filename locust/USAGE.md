# ğŸš€ Locust Load Testing - KullanÄ±m KÄ±lavuzu

## âš¡ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Kurulum
```bash
cd locust
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Locust Web UI ile Test (ğŸ‘‘ Recommended)
```bash
source venv/bin/activate
locust -f locustfile.py --host http://34.22.249.41:30080
```

Sonra tarayÄ±cÄ±da http://localhost:8089 adresine gidin:
- **Number of users**: 20-50 (baÅŸlangÄ±Ã§ iÃ§in)
- **Spawn rate**: 5-10 users/second
- **Run time**: 5m (300s)

**ğŸ‘€ CanlÄ± Monitoring iÃ§in:**
- **Statistics** tab'Ä±: Real-time request stats
- **Charts** tab'Ä±: Response time graphs
- **Failures** tab'Ä±: Error analysis
- **Logs** tab'Ä±: Detailed logs

### 3. Headless Test (Command Line)
```bash
# 10 kullanÄ±cÄ±, 2/saniye spawn, 1 dakika
source venv/bin/activate
locust -f locustfile.py --host http://34.22.249.41:30080 -u 10 -r 2 -t 60s --headless --only-summary
```

## ğŸ¯ HPA Scaling Testleri

### Otomatik Script ile
```bash
# Default: 50 user, 10/sec spawn, 5 dakika
./hpa-test.sh

# Ã–zelleÅŸtirilmiÅŸ
./hpa-test.sh 100 20 600  # 100 user, 20/sec spawn, 10 dakika
```

### Manuel HPA Test
```bash
source venv/bin/activate

# 1. Normal load test
locust -f locustfile.py --host http://34.22.249.41:30080 -u 50 -r 10 -t 300s --headless --only-summary

# 2. CPU yoÄŸun test (daha hÄ±zlÄ± scaling iÃ§in)
locust -f locustfile.py CPUIntensiveUser --host http://34.22.249.41:30081 -u 30 -r 5 -t 300s --headless --only-summary
```

## ğŸ“Š Test SenaryolarÄ±

### 1. Realistic User Flow (TodoAppUser)
- âœ… User registration/login
- âœ… Dashboard viewing  
- âœ… Todo CRUD operations
- âœ… AI insights integration
- âœ… Token verification

### 2. CPU Intensive Load (CPUIntensiveUser)
- âœ… High-frequency API calls
- âœ… Auth service stress testing
- âœ… Todo service stress testing 
- âœ… HPA trigger optimization

## ğŸ” Monitoring HPA Scaling

Test Ã§alÄ±ÅŸtÄ±rÄ±rken baÅŸka terminal'de:

```bash
# HPA durumunu sÃ¼rekli izle
watch kubectl get hpa -n todo-app

# Pod scaling'ini izle  
watch kubectl get pods -n todo-app

# CPU metrics
kubectl top pods -n todo-app
```

## ğŸ¯ Test Targets

### Current Production URLs:
- **Frontend**: http://34.22.249.41:30080
- **Auth API**: http://34.22.249.41:30081  
- **Todo API**: http://34.22.249.41:30082
- **AI Insights**: https://todo-app-insights-dev-tbv5uyb5va-ew.a.run.app

### Expected Performance:
- **Response Time**: < 200ms (p95)
- **Success Rate**: > 99%
- **HPA Trigger**: 50% CPU
- **Scale Range**: 1-5 pods

## ğŸš¨ Best Practices

### Load Testing Guidelines:
```bash
# BaÅŸlangÄ±Ã§ seviyesi (ilk test)
-u 10 -r 2 -t 60s

# Orta seviye (normal kullanÄ±m)
-u 25 -r 5 -t 300s

# YÃ¼ksek seviye (stress test)
-u 50 -r 10 -t 600s

# Ekstrem seviye (HPA trigger)
-u 100 -r 20 -t 300s
```

### Rate Limiting removed:
âœ… Auth service rate limiting devre dÄ±ÅŸÄ±
âœ… Todo service rate limiting devre dÄ±ÅŸÄ±
âœ… Maksimum throughput iÃ§in optimize edildi

### Error Handling:
- **409 Conflict**: Normal (user already exists)
- **404 Logout**: Normal (endpoint yok)
- **401 Unauthorized**: Data validation problemi
- **429 Too Many Requests**: Rate limiting (Ã§Ã¶zÃ¼ldÃ¼)

## ğŸ”§ Troubleshooting

### Common Issues:

1. **Username validation error**:
   - âœ… Ã‡Ã¶zÃ¼ldÃ¼: Alphanumeric only usernames

2. **Network timeouts**:
   ```bash
   # Reduce spawn rate: -r 2 instead of -r 10
   # Reduce concurrent users: -u 10 instead of -u 50
   ```

3. **Memory issues**:
   ```bash
   # HPA scaling devreye girerse normal
   kubectl get hpa -n todo-app
   ```

## ğŸ“ˆ Success Metrics

### âœ… Current Achievement:
- **Success Rate**: 99.58%
- **Response Time**: p95 < 160ms  
- **HPA Scaling**: âœ… Working (1â†’3 pods)
- **Throughput**: 7.88 req/s (with 5 users)
- **Zero Rate Limiting**: âœ… Removed

### ğŸ¯ Scale Test Results:
```
Auth Service: 28% CPU â†’ 3 pods
Todo Service: 8% CPU â†’ 3 pods
Threshold: 50% CPU
Max pods: 5
```

## ğŸ“‹ Test Checklist

Before running load tests:

- [ ] All pods are running: `kubectl get pods -n todo-app`
- [ ] Services are healthy: `curl http://34.22.249.41:30081/health`
- [ ] HPA is configured: `kubectl get hpa -n todo-app`
- [ ] Metrics server is running: `kubectl get pods -n kube-system | grep metrics`

During tests:

- [ ] Monitor CPU usage
- [ ] Watch pod scaling events
- [ ] Check error rates
- [ ] Observe response times

After tests:

- [ ] Review HPA events: `kubectl describe hpa -n todo-app`
- [ ] Check pod logs for errors
- [ ] Verify system returned to baseline
- [ ] Document performance metrics

## ğŸš¨ Troubleshooting

### Common Issues:

**429 Too Many Requests**
```bash
# Rate limiting aktif - normal davranÄ±ÅŸ
# Reduce spawn rate: -r 2 instead of -r 10
```

**Connection Refused**
```bash
kubectl get pods -n todo-app
kubectl get services -n todo-app
```

**HPA Not Scaling**
```bash
kubectl describe hpa todo-app-auth-hpa -n todo-app
kubectl top pods -n todo-app
```

**High Error Rates**
```bash
kubectl logs -f deployment/todo-app-auth -n todo-app
kubectl logs -f deployment/todo-app-todo -n todo-app
```

## ğŸ‰ Success Metrics

A successful HPA test should show:

âœ… **CPU Scaling**: 10% â†’ 60% â†’ back to 10%  
âœ… **Pod Scaling**: 1 â†’ 2/3 â†’ back to 1  
âœ… **Response Times**: <2000ms under load  
âœ… **Error Rates**: <10% during peak  
âœ… **System Recovery**: Clean scale-down  

## ğŸ”— Useful Commands

```bash
# Quick status check
kubectl get pods,hpa -n todo-app

# Detailed HPA info
kubectl describe hpa todo-app-auth-hpa -n todo-app

# Resource usage
kubectl top pods -n todo-app --sort-by=cpu

# Event logs
kubectl get events -n todo-app --sort-by=.metadata.creationTimestamp

# Complete monitoring
watch -n 2 'echo "=== PODS ===" && kubectl get pods -n todo-app && echo "=== HPA ===" && kubectl get hpa -n todo-app && echo "=== CPU ===" && kubectl top pods -n todo-app'
``` 